基础数学知识：
（1）分布：二项分布、多项分布是指，在某事件结果的概率分布已知的情况下，多次重复该事件，得到的统计结果将会服从的概率分布。
          而Beta分布和Dirichlet分布是指，重复某事件并获取事件结果的统计，进而估计得到的该事件结果的概率分布的概率分布。所以，可以从Beta分布和Dirichlet分布中抽样获取到事件结果的概率分布。
（2）采样：吉布斯采样是马尔可夫过程采样的优化版本，可以在不获取平稳转移概率矩阵的情况下，使用条件概率，进行快速的采样。

按照以下方式生成文档（模型原理）：
（1）存在两个Dirichlet分布，一个是关于Doc-Topic的，记为Dirichlet1；另一个是关于Topic-Word的，记为Dirichlet2。
（2）首先从Dirichlet2从抽样K次，得到K个Topic-Word多项分布。
（3）从Dirichlet1中抽样生成一篇文档的Doc-Topic多项分布，根据该分布抽样生成当前词的主题，记为T。然后根据主题T的Topic-Word多项分布，抽样生成具体的词。循环该过程，直至生成所有文档。

训练原理：
（1）随机初始化文章中每个词所属的主题，使用吉布斯采样，重新获取每个词的主题。不断重复直至过程平稳后，获取Doc-Topic和Topic-Word的频率共现矩阵，进而得到两个Dirichlet分布的参数（超参数需人为指定）。
（2）根据吉布斯采样的要求，必须有条件概率用以进行状态的转移。
     这个条件概率可以通过统计获得，根据吉布斯采样的公式可知，在统计过程中需要排除当前词对计数的影响（仅指当前位置的这个词，其他位置的同一个词不需要被排除计数）。
     具体推导过程不表，在此仅做一个通俗解释：文章D中，词W为主题T的条件概率等于=词W在主题T中的后验概率*主题T在文章D中的后验概率。
（3） 一般并不判断是否已完全收敛，而是通过足够多的训练步数，来达到平稳状态的要求。

LDA的使用：
（1）训练好的LDA模型，主要用于推断新文档的主题，以及获取新的Topic-Word多项分布。所以训练得到的关于Doc-Topic的内容（包括对应的Dirichlet分布和频率共现矩阵）一般不需要保留。
（2）因为需要在不更换Topic的情况下，推断新文档的主题，所以训练得到的Topic-Word频率共现矩阵是需要保留的。
（3）因为可能需要尝试新的主题，所以关于Topic-Word的Dirichlet分布的参数也需要保留。
