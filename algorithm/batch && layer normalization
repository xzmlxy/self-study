batch normalization 改变了输入数据的分布，把数据从原分布映射到了新的分布。这并不会影响训练出来的模型的性能，只是输入数据的分布改变了而已，
同时，分布的改变，有效的加快了训练的速BN度，防止了梯度消失等现象的出现，这就是 BN 的初衷。
layer normalization 和 BN 类似，不同的是它对一个样本的不同特征做了normalization，这要求不同特征的量纲应该是相同的。NLP任务显然满足上述要求，词向量的每个特征的量纲是相同的。
之所以要在NLP以及类似序列任务上使用LN，是因为序列的长度长短不一，如果使用 BN，太长的测试语句的尾部，可能不存在对应的训练均值和方差。即使存在，该值也可能因为统计数据过少而不够正确。
