
后馈算法本质上就是对所有参数使用梯度下降。即对某个参数Wn来说，更新后的值为Wn+ηΔWn，其中ΔWn=(∂Etotal)/(∂Wn)。
整个计算过程和前向计算是相反的，所以称为后馈。后馈的好处在于，对于前向第n层的计算，可以利用前向第n+1层计算的某些中间值，进而减少计算量。
在进行后馈计算时，无论是那一层，都只使用之前前馈时的参数和数据，不会使用任何当前轮更新后的参数或数据。
对于RNN，同样可以利用这种后馈方式来更新参数（BPTT），把时间步长度为n的RNN看作是同样深度的DNN即可。唯一不同的是，需要把所有时间步上ΔWn进行叠加后，再更新参数。
由于RNN的时间步长度一般比较大，所以RNN更容易受到梯度消失或梯度爆炸的影响，因此需要LSTM等一系列的改进。
