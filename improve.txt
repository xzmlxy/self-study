import pandas as pd
from code.models.XGBModel import XGBModel
from code.models.LightGBModel import LightGBModel
from code.models.CatBModel import CatBModel
from code.models.StackingModel import StackingModel
from sklearn.metrics import cohen_kappa_score
from code.DataUtil import DataUtil


class ModelAgent:
    def __init__(self, data_x, data_y, one_model=False):
        self.model_dic = {}
        self.one_model = one_model
        self.cat_features = []
        for name in data_x.columns.values:
            if name.startswith('world_name'):
                self.cat_features.append(name)
        if not one_model:
            assessments = data_x['title'].unique()
            for assessment in assessments:
                train_x = data_x[data_x['title'] == assessment]
                train_y = data_y[data_y['game_session'].isin(train_x['game_session'])]
                combine_x_y = pd.merge(train_x, train_y, on='game_session')
                self.model_dic[assessment] = self.create_model(combine_x_y)
        else:
            for name in DataUtil.assessment_names:
                self.cat_features.append('title_name_' + name)
            data_x['title_name'] = data_x['title']
            data_x = pd.get_dummies(data_x, columns=['title_name'])
            combine_x_y = pd.merge(data_x, data_y, on='game_session')
            self.model_dic['all'] = self.create_model(combine_x_y)

    def create_model(self, data):
        data.drop(columns=['installation_id', 'title', 'game_session'], inplace=True)
        XGBM = XGBModel(data, y_name='accuracy_group')
        LGBM = LightGBModel(data, y_name='accuracy_group')
        CBM = CatBModel(data, y_name='accuracy_group')
        CBM.set_cat_features(self.cat_features)
        stacking_model = StackingModel([XGBM, LGBM, CBM])
        return LGBM

    def train(self):
        for _, model in self.model_dic.items():
            if model is not None:
                model.train()

    def adjust_model_params(self):
        for _, model in self.model_dic.items():
            if model is not None:
                model.adjust_params_one_by_one()

    def get_current_model_cv_cohen_kappa_score(self, cv):
        score_dict = {}
        for name, model in self.model_dic.items():
            if model is not None:
                value = model.get_cv_score(cv, cohen_kappa_score)
                score_dict[name] = value
        return score_dict

    def predict(self, test_data):
        if not self.one_model:
            all_results = []
            for assessment in self.model_dic:
                part_test_data = test_data[test_data['title'] == assessment]
                part_test_data.reset_index(drop=True)
                predict_result = self.model_dic[assessment].predict(
                    part_test_data.drop(columns=['installation_id', 'title', 'game_session']))
                if predict_result is not None:
                    result = pd.DataFrame(
                        {'installation_id': part_test_data['installation_id'].values, 'accuracy_group': predict_result})
                    all_results.append(result)
            return pd.concat(all_results)
        else:
            test_data['title_name'] = test_data['title']
            test_data = pd.get_dummies(test_data, columns=['title_name'])
            predict_result = self.model_dic['all'].predict(
                test_data.drop(columns=['installation_id', 'title', 'game_session']))
            return pd.DataFrame(
                {'installation_id': test_data['installation_id'].values, 'accuracy_group': predict_result})
                
                

import numpy as np
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from abc import ABCMeta, abstractmethod


class BaseModel(metaclass=ABCMeta):
    def __init__(self, data, y_name: str = None, params: dict = None):
        self.data = shuffle(data).reset_index(drop=True)
        self.params = params
        self.threshold = 100
        self.model = None
        self.selection = None
        self.y_name = y_name
        if self.y_name is None:
            raise Exception()
        self.x_train, self.y_train = self.__split_x_y(self.data)

    def __split_x_y(self, data):
        x = data[[column_name for column_name in data.columns if column_name != self.y_name]]
        y = data[self.y_name]
        return x, y

    def set_select_feature_threshold(self, threshold):
        self.threshold = threshold

    def train(self):
        print('Training...')
        x_train, x_test, y_train, y_test = train_test_split(self.x_train, self.y_train, test_size=0.1, random_state=6)
        self.model, self.selection = self._internal_train(x_train, y_train, x_test, y_test)
        print('Finish to train.')

    def predict(self, data):
        if self.model is not None and self.selection is not None:
            select_x_train = self.selection.transform(data)
            return self.model.predict(select_x_train)
        else:
            print("No model to  predict!")
            return None

    def _internal_train(self, x_train, y_train, x_test, y_test):
        model = self._get_model(x_train, y_train, x_test, y_test)
        thresholds = np.sort(model.feature_importances_)
        selection = SelectFromModel(model, threshold=thresholds[-self.threshold], prefit=True)
        x_train = selection.transform(x_train)
        if not x_test is None:
            x_test = selection.transform(x_test)
        model = self._get_model(x_train, y_train, x_test, y_test)
        return model, selection

    @abstractmethod
    def _get_model(self, x_train, y_train, x_test, y_test):
        return None

    def get_cv_score(self, cv, loss_fun):
        result = np.ndarray(cv)
        skf = StratifiedKFold(n_splits=cv)
        i = 0
        x, y = self.__split_x_y(self.data)
        for train, test in skf.split(x, y):
            x_train = x.iloc[train]
            y_train = y.iloc[train]
            x_test = x.iloc[test]
            y_test = y.iloc[test]
            model, selection = self._internal_train(x_train, y_train, x_test, y_test)
            select_x_train = selection.transform(x_test)
            predict_y = model.predict(select_x_train)
            result[i] = loss_fun(predict_y, y_test)
            print(result[i])
            i += 1
        return result.mean()

    def adjust_params(self, one_by_one=True):
        params = self.create_adjust_params()
        if params is not None:
            self.train()
            if one_by_one:
                self._internal_adjust_params_one_by_one(params)
            else:
                self._internal_adjust_param(params)
            print(self.params)


    @abstractmethod
    def create_adjust_params(self) -> dict:
        return {}

    def _internal_adjust_params_one_by_one(self, params):
        for key, value_range in params.items():
            self._internal_adjust_param({key: value_range})

    def _internal_adjust_param(self, adjust_param):
        if self.model is None:
            print("No model to adjust!")
        else:
            my_cv = StratifiedKFold(n_splits=5)
            print('Adjusting...')
            gs = GridSearchCV(self.model, adjust_param, cv=my_cv)
            gs.fit(self.extra_pre_process(self.x_train), self.extra_pre_process(self.y_train))
            self.params.update(gs.best_params_)
            print('Finish to adjust.')
            print("参数的最佳取值：", gs.best_params_)

    def extra_pre_process(self, data):
        return data
        
        
        class XGBModel(BaseModel):
    XGB_params = {'n_estimators': np.linspace(100, 500, 5, dtype=int),
                  'max_depth': np.linspace(5, 9, 5, dtype=int),
                  'min_child_weight': np.linspace(1, 5, 3, dtype=int),
                  'reg_alpha': np.linspace(0, 1, 11, dtype=float),
                  'reg_lambda': np.linspace(0, 1, 11, dtype=float),
                  'gamma': np.linspace(0, 0.3, 7, dtype=float),
                  'learning_rate': np.linspace(0.05, 0.15, 3, dtype=float),
                  'num_parallel_tree': np.linspace(0, 100, 11, dtype=int), }

    def __init__(self, data, y_name: str = None, params: dict = None):
        super().__init__(data, y_name, params)
        if self.params is None:
            self.params = {'learning_rate': 0.1, 'objective': 'multi:softmax', 'num_class': 4,
                           'n_estimators': 200, 'max_depth': 8, 'reg_alpha': 0, 'reg_lambda': 1, 'gamma': 0,
                           'min_child_weight': 1, 'num_parallel_tree': 1}

    def _get_model(self, x_train, y_train, x_test, y_test):
        model = xgboost.XGBRFClassifier(**self.params)
        if x_test is not None:
            model.fit(x_train, y_train, eval_metric='mlogloss', early_stopping_rounds=10, eval_set=[(x_test, y_test)])
        else:
            model.fit(x_train, y_train)
        return model

    def create_adjust_params(self):
        return XGBModel.XGB_params



class LightGBModel(BaseModel):
    LightGB_params = {'n_estimators': np.linspace(100, 500, 5, dtype=int),
                      'max_depth': np.linspace(5, 9, 5, dtype=int),
                      'min_child_weight': np.linspace(0.001, 0.002, 2, dtype=float),
                      'reg_alpha': np.linspace(0, 1, 11, dtype=float),
                      'reg_lambda': np.linspace(0, 1, 11, dtype=float),
                      'num_leaves': np.linspace(32, 96, 3, dtype=int),
                      'min_child_samples': np.linspace(18, 22, 3, dtype=int),
                      'feature_fraction': np.linspace(0.8, 1, 3, dtype=float),
                      'learning_rate': np.linspace(0.05, 0.15, 3, dtype=float)}

    def __init__(self, data, y_name: str = None, params: dict = None):
        super().__init__(data, y_name, params)
        if self.params is None:
            self.params = {'learning_rate': 0.1, 'objective': 'multiclass', 'num_class': 4, 'n_estimators': 100,
                           'num_leaves': 32, 'max_depth': 5, 'min_child_weight': 0.001, 'min_child_samples': 20,
                           'reg_alpha': 0.3,'reg_lambda': 0.2, 'feature_fraction': 0.9}

    def _get_model(self, x_train, y_train, x_test, y_test):
        x_train = self.extra_pre_process(x_train)
        x_test = self.extra_pre_process(x_test)
        model = lgb.LGBMClassifier(**self.params)
        if x_test is not None:
            model.fit(x_train, y_train, eval_metric='logloss', early_stopping_rounds=32, eval_set=[(x_test, y_test)])
        else:
            model.fit(x_train, y_train)
        return model

    def create_adjust_params(self):
        return LightGBModel.LightGB_params

    def extra_pre_process(self, data):
        return np.array(data)
        
        
    class CatBModel(BaseModel):
    CatB_params = {'learning_rate': np.linspace(0.05, 0.15, 3, dtype=float),
                   'iterations': np.linspace(100, 500, 5, dtype=int),
                   'depth': np.linspace(5, 9, 5, dtype=int),
                   'l2_leaf_reg': np.linspace(0, 1, 11, dtype=float),
                   'min_data_in_leaf': np.linspace(10, 20, 6, dtype=int),
                   'fold_len_multiplier': np.linspace(1.2, 2, 5, dtype=float),
                   'border_count': np.linspace(64, 128, 2, dtype=int)}

    def __init__(self, data, y_name: str = None, params: dict = None):
        super().__init__(data, y_name, params)
        self.cat_features = []
        if self.params is None:
            self.params = {'learning_rate': 0.1, 'iterations': 100, 'depth': 6, 'classes_count': 4,
                           'loss_function': 'MultiClass', 'min_data_in_leaf': 10, 'od_type': 'Iter',
                           'fold_len_multiplier': 1.6, 'border_count': 128, 'task_type': 'GPU'}

    def _get_model(self, x_train, y_train, x_test, y_test):
        x_train = self.extra_pre_process(x_train)
        x_test = self.extra_pre_process(y_test)
        if x_test is not None:
            self.params['early_stopping_rounds'] = 50
            model = catboost.CatBoostClassifier(**self.params, cat_features=self.cat_features)
            model.fit(x_train, y_train, eval_set=[(x_test, y_test)], use_best_model=True)
        else:
            model = catboost.CatBoostClassifier(**self.params)
            model.fit(x_train, y_train)
        return model

    def set_cat_features(self, features):
        self.cat_features.extend(features)

    def create_adjust_params(self):
        return CatBModel.CatB_params

    def extra_pre_process(self, data):
        for name in self.cat_features:
            data[name] = data[name].apply(int)
        return data



from sklearn.base import BaseEstimator, TransformerMixin, clone
from sklearn.model_selection import KFold
import numpy as np

class StackingModel:
    def __init__(self, models,out_model):
        self.models = models
        self.out_model = out_model

    def train(self):
        pass

    def predict(self):
        pass


class StackingAveragedModels (BaseEstimator, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds

    # We again fit the data on clones of the original models
    def fit(self, X, y):
        self.base_models_ = [list () for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)

        # 使用K-fold的方法来进行交叉验证，将每次验证的结果作为新的特征来进行处理
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index],  y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred

        # 将交叉验证预测出的结果 和 训练集中的标签值进行训练
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self

    # 从得到的新的特征  采用新的模型进行预测  并输出结果
    def predict(self, X):
        meta_features = np.column_stack ([
            np.column_stack([model.predict (X) for model in base_models]).mean (axis=1)
            for base_models in self.base_models_])
        return self.meta_model_.predict(meta_features)

#stacked_averaged_models = StackingAveragedModels(base_models=(ENet, GBoost, KRR),meta_model=lasso)
