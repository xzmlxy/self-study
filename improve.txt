from sklearn.base import BaseEstimator, clone
from sklearn.model_selection import StratifiedKFold
import numpy as np
import copy


class StackingModel(BaseEstimator):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.base_meta_model = meta_model
        self.n_folds = n_folds
        self.create_models = None
        self.create_meta_model = None

    def fit(self, x, y, early_stop=True):
        self.create_models = [list() for _ in self.base_models]
        self.create_meta_model = clone(self.base_meta_model)
        skf = StratifiedKFold(n_splits=self.n_folds, random_state=10)
        # 使用K-fold的方法来进行交叉验证，将每次验证的结果作为新的特征来进行处理
        out_of_fold_predictions = np.zeros((x.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in skf.split(x, y):
                instance = copy.deepcopy(model)
                self.create_models[i].append(instance)
                instance.fit(x[train_index], y=y[train_index], need_early_stop=early_stop)
                y_predict = instance.predict(x[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_predict
        self.create_meta_model.fit(out_of_fold_predictions, y)
        return self

    def predict(self, x_test):
        meta_features = np.column_stack([np.column_stack([model.predict(x_test) for model in models_of_same_type]).mean(axis=1) for models_of_same_type in self.create_models])
        return self.create_meta_model.predict(meta_features)   
        
 
 from code.models.ensemble_models.StackingModel import StackingModel
from code.models.BaseModel import BaseModel
import lightgbm as lgb
from code.models.LightGBModel import LightGBModel
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
import numpy as np


class StackModelWrapper(BaseModel):
    def __init__(self, models: list, y_name: str = None, params: dict = None, reg: bool = False,
                 model_class_name: str = 'LGB'):
        super().__init__(y_name, params)
        self.models = models
        if not self.__check_base_model():
            print('The sub model is not base model!')
            raise Exception
        self.model_class_name = model_class_name
        if model_class_name == 'LGB':
            if reg:
                if self.params is None:
                    self.model = lgb.LGBMRegressor()

                else:
                    self.model = lgb.LGBMRegressor(**self.params)
            else:
                if self.params is None:
                    self.model = lgb.LGBMClassifier()
                else:
                    self.model = lgb.LGBMClassifier(**self.params)
            self.adjust_params = LightGBModel.LightGB_params
        elif model_class_name == 'TREE':
            if reg:
                if self.params is None:
                    self.model = DecisionTreeRegressor()
                else:
                    self.model = DecisionTreeRegressor(**self.params)
            else:
                if self.params is None:
                    self.model = DecisionTreeClassifier()
                else:
                    self.model = DecisionTreeClassifier(**self.params)
            self.adjust_params = {'max_depth': np.linspace(5, 9, 5, dtype=int),
                                  'max_leaf_nodes': np.linspace(32, 128, 4, dtype=int)}
        else:
            print('Sorry,we will add more models in future.')

    def __check_base_model(self):
        for mode in self.models:
            if not isinstance(mode, BaseModel):
                return False
        return True

    def _get_model(self, x_train, y_train, x_test, y_test):
        if self.model is not None and self.models is not None:
            model = StackingModel(self.models, self.model)
            model.fit(x_train, y_train, early_stop=True)
            return model

    def fit(self, data, y=None, need_early_stop=True, select=True):
        super().fit(data, y, False, False)

    def create_adjust_params(self):
        return self.adjust_params
        
        
        
        
            def fit(self, data, y=None, need_early_stop=True, select=False):
        if y is None:
            x_train, y_train = self.__split_x_y(data)
        else:
            x_train = data
            y_train = y
            
            
            
            
                        self.params = {'objective': 'multi:softmax', 'num_class': 4, 'n_estimators': 400, 'min_child_weight': 2,
                           'max_depth': 9, 'reg_alpha': 0.5, 'reg_lambda': 0.1, 'gamma': 0.2, 'learning_rate': 0.1}
                           
                           
                           
                               XGB_params = {'n_estimators': np.linspace(100, 500, 4, dtype=int),
                  'max_depth': np.linspace(6, 10, 5, dtype=int),
                  'min_child_weight': np.linspace(1, 5, 5, dtype=int),
                  'reg_alpha': np.linspace(0, 0.8, 9, dtype=float),
                  'reg_lambda': np.linspace(0, 0.8, 9, dtype=float),
                  'gamma': np.linspace(0, 0.3, 3, dtype=float),
                  'learning_rate': np.linspace(0.05, 0.15, 3, dtype=float)}

